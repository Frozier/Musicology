---
title: "Storyboard"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: rows
---

```{r, eval = FALSE, echo=FALSE}
remotes::install_github('jaburgoyne/compmus')
```

<style>
.navbar-inverse {
  background-color: #222;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav>li>a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav>.active>a {
  background-color: #222;
}
.list-group-item.active {
  background-color: #222;
  border-color: #222;
}
item.active, .list-group-item.active:hover {
  background-color: #222;
  border-color: #222;
}
</style>

```{r libraries, message=FALSE, echo=FALSE}
library(ggplot2)
library(tidyverse)
library(spotifyr)
library(extrafont)
library(plotly)
library(compmus)
```

Introduction {.storyboard}
========================================

### K-pop, J-pop and Western

#### Tracks description

The corpus is a playlist of songs picked from various bands from different countries/regions in the world. I chose this corpus because I wanted to learn more about the differences in music styles from different regions. The bands that are included are RADWIMPS, The Script, ONE OK ROCK and Day6.

The songs are picked such that they all follow somewhat the same trend in terms of genre, namely pop (rock), instead of mixing it with genres such as heavy metal. Apart from the regional differences, there are other interesting things about the included songs. RADWIMPS and ONE OK ROCK for example are both bands that originate from Japan, however one of them has western influences such as mainly singing in English, and the other has some songs that are written for films, though they all have vocals and still follow a similar genre as the other bands. The Script is an Irish band and Day6 originates from South-Korea.

Even though bands and artists from different regions in the world can write music in similar genres, their music styles are often considered to be noticeably different, and artists themselves may be very aware of this. One example is how the vocals are used for the melodies, or the difference in instrumentation. For the purposes of this project weâ€™re analyzing music mainly based on geographic location and how music styles differ or resemble.


Analyses {.storyboard}
========================================


### Overall 'feel' of the different tracks

```{r}
# Change to Georgia font family pls
musicology <- get_playlist_audio_features("", "4qytaIOVyWpUF2ncTXkjLZ") %>%
  mutate(artist.name = map_chr(track.artists, function(x) x$name[1])) %>%
  mutate(valence_factor = round(valence))
```

```{r}
# plot with energy and danceability
energy_dance <- ggplot(musicology, aes(energy, danceability, color = artist.name)) +
  geom_point(size = 5, alpha = 0.4) +
  theme(text = element_text(family = "Bookman", color = "Gray25")) +
  labs(title = "Artists show different distributions in energy and danceability",
       y = "Danceability", 
       x = "Energy",
       subtitle = "The Script shows a lot of diversity") +
  guides(color = guide_legend(title = "Artist"))
#energy_dance
ggplotly(energy_dance)
```

***
When we analyze the danceability and energy of the tracks, there are noticeable differences between each of the artists. Notice that the figure shown here is limited in the axis as all the tracks group well within the current ranges.

For this analysis, The Script takes it all. Many of their tracks are spread between low and high energy _and_ danceability, thus showing a lot of variation. RADWIMPS skews towards a not-too-high-not-too-low danceability and energy whereas ONE OK ROCK shows much higher energy levels. These difference in energy between the two artists may stem from the genre, where ONE OK ROCK, as the name implies, plays many rock songs unlike RADWIMPS does. The danceability between them are similar however.

Day6 follows a line with low energy and danceability in their tracks, to tracks with high levels in both.



### Differences between western and eastern music

```{r}
musicology |>                    # All tracks
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) |>
  ggplot(                     # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = loudness,
      colour = mode
    )
  ) +
  geom_point() +              # Scatter plot.
  geom_rug(linewidth = 0.1) + # Add 'fringes' to show data distribution.
  geom_text(                  # Add text labels from above.
    aes(
      x = valence,
      y = energy,
      label = ""
    ),
    colour = "black",         # Override colour (not mode here).
    size = 3,                 # Override size (not loudness here).
    hjust = "left",           # Align left side of label with the point.
    vjust = "bottom",         # Align bottom of label with the point.
    nudge_x = -0.05,          # Nudge the label slightly left.
    nudge_y = 0.02            # Nudge the label slightly up.
  ) +
  facet_wrap(~artist.name) +    # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Paired"        # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasise loud.
    guide = "none"            # Remove the legend for size.
  ) +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    title = "Positivity between artists",
    x = "Valence",
    y = "Energy",
    colour = "Mode"
  )
```

***
Many of the artists choose for a major modality in their tracks, with DAY6 showing more of the minor kind. Furthermore the loudness seem to be similar across all artists, but we now have valence which does differ for each.

In this case The Script again shows a lot of variation in valence, but RADWIMPS and ONE OK ROCK skew more toward lower valence, although their tracks with higher valence are also noticeably louder. DAY6 shows a similar trend, and it may point to these three artists being similar because they are in the same continent, whereas The Script is on the other side of the world.

### DTW between similar songs

```{r}
## The Tallis Scholars
science_faith <-
  get_tidy_audio_analysis("0cnouzAiEjdjXB5xVVQ8Vo") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
## La Chapelle Royale
wasted_nights <-
  get_tidy_audio_analysis("39LyUQIy7idLxPdsjyZsxe") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


compmus_long_distance(
  science_faith |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  wasted_nights |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Science & Faith", y = "Wasted Nights", title = "Dynamic Time Warping between two similar songs", subtitle = "similarity based on energy and danceability") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL)
```

***
For a next analysis I decided to pick two songs from the "overal feel" plot that were similar in energy and danceability, these songs being Science & Faith (The Script) and Wasted Nights (ONE OK ROCK). Despite my expectations there seems to be solely dissimilarity between them, and there's hardly any vector or line present that displays anything meaningful. While Dynamic Time Warping can help understanding the difference between chroma vectors in two songs, it'll be important to know where and what songs to look into for proper analysis. What I've shown in this figure suggests that energy and danceability are perhaps not a major factor in regional difference, but rather similarity.


### Chromagram of We'll Be Alright

```{r, out.extra=c('allow="encrypted-media"', 'allowtransparency="true"', 'frameBorder="0"')}
alright <-
  get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

alright_timbre <-
  get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
```

```{r}
alright |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

***
In this part we will discuss one of the outliers from our first data plot. We'll Be Alright was written by RADWIMPS and shows a quite low level of danceability, with a value of 0.174, compared to all the other tracks. What is interesting to see is the overal distribution of the pitches; there is not so much a dominance. The next plot will show a timbregram which looks quite different.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7vSF7u4vWtZGrWCxTbAVaw?utm_source=generator&theme=0" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy" data-external="1"></iframe>
```

### Timbregram of We'll Be Alright

```{r}
alright_timbre |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

***
In the timbregram for We'll Be Alright we can see a dominance in magnitude toward pitches c01-c03. This is vastly different from the chromagram.


### Self-Similarity Matrices

```{r hazes}
alright <-
  get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  alright |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  alright |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***
The chroma-based self-similarity matrix of We'll Be Alright shows something very interesting. There is nearly no similarity between any of the parts within the song!

Naturally you'd expect songs in general to show some level of similarity, or rather repetition, in different parts of the song. Do not be mistaken however, this is nothing like the case with a song like Bohemian Rhapsody, where the entire structure has little pattern. Rather, We'll Be Alright shows an incredible amount of variation while maintaining a very clear structure in the parts. For example, the first and the second chorus have the same melody, but the instrumentation and rhythm are entirely different.

On one hand it's unwise to identify it as an outlier since it still has strong pop music themes. On the other hand it's also an outlier of RADWIMPS themselves, and it's hardly representative for what their music is generally like. However that's something to look for in a manual analysis too.

### Chordogram

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r}
alright <-
  get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
alright |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***
Here we have a chordogram of the song We'll Be Alright. There's notably something interesting happening later on in the song. For the most part the chords look consistent. But if you listen to the time frame at around 200 seconds into the song, you will hear a very short change in key. In fact it changes twice there and serves as an embellishment.


### Variation (standard deviation)

```{r}
RADWIMPS <-
  musicology %>%
  filter(artist.name == "RADWIMPS") |>
  slice(1:30) |>
  add_audio_analysis()
thescript <-
  musicology %>%
  filter(artist.name == "The Script") |>
  slice(1:30) |>
  add_audio_analysis()
ONEOKROCK <-
  musicology %>%
  filter(artist.name == "ONE OK ROCK") |>
  slice(1:30) |>
  add_audio_analysis()
DAY6 <-
  musicology %>%
  filter(artist.name == "DAY6") |>
  slice(1:30) |>
  add_audio_analysis()
pop <-
  RADWIMPS |>
  mutate(genre = "RADWIMPS") |>
  bind_rows(thescript |> mutate(genre = "The Script")) |>
  bind_rows(ONEOKROCK |> mutate(genre = "ONE OK ROCK")) |>
  bind_rows(DAY6 |> mutate(genre = "DAY6"))

pop |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections) |>
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = genre,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Genre",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )
```

***
description is in the works


Conclusion {.storyboard}
========================================


### What makes each region's music unique

```{r}
```

A lot can differ in music from different regions of the world, but similarities also show a good resemblance between artists and their music, and it makes it easier to judge what really makes the difference. We've seen that some artists prefer composing songs in major, and one in minor. We've also seen how valence, energy and danceability show different variations in each artist (which may point to the major differences!). What we can analyze with the available data is however quite limited; plently of features didn't show interesting numbers that affect how we see the music, and we don't have features with song structure and rhythm.

This conclusion/discussion page is still a work in progress