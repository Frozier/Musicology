---
title: "Storyboard"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: rows
---

```{r, eval = FALSE, echo=FALSE}
remotes::install_github('jaburgoyne/compmus')
```

<style>
.navbar-inverse {
  background-color: #222;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav>li>a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav>.active>a {
  background-color: #222;
}
.list-group-item.active {
  background-color: #222;
  border-color: #222;
}
item.active, .list-group-item.active:hover {
  background-color: #222;
  border-color: #222;
}
</style>

```{r libraries, message=FALSE, echo=FALSE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(spotifyr)
library(extrafont)
library(plotly)
library(compmus)
#library(tidymodels)
#library(ggdendro)
#library(heatmaply)
```

Introduction {.storyboard}
========================================

### K-pop, J-pop and Western

#### Tracks description

The corpus is a playlist of songs picked from various bands from different countries/regions in the world. I chose this corpus because I wanted to learn more about the differences in music styles from different regions. The bands that are included are RADWIMPS, The Script, ONE OK ROCK and Day6.

The songs are picked such that they all follow somewhat the same trend in terms of genre, namely pop (rock), instead of mixing it with genres such as heavy metal. Apart from the regional differences, there are other interesting things about the included songs. RADWIMPS and ONE OK ROCK for example are both bands that originate from Japan, however one of them has western influences such as mainly singing in English, and the other has some songs that are written for films, though they all have vocals and still follow a similar genre as the other bands. The Script is an Irish band and Day6 originates from South-Korea.

Even though bands and artists from different regions in the world can write music in similar genres, their music styles are often considered to be noticeably different, and artists themselves may be very aware of this. One example is how the vocals are used for the melodies, or the difference in instrumentation. For the purposes of this project we’re analyzing music mainly based on geographic location and how music styles differ or resemble.


Analyses {.storyboard}
========================================


### Overall 'feel' of all the tracks

```{r}
# Change to Georgia font family pls
musicology <- get_playlist_audio_features("", "4qytaIOVyWpUF2ncTXkjLZ") %>%
  mutate(artist.name = map_chr(track.artists, function(x) x$name[1]))
```

```{r}
# plot with energy and danceability
energy_dance <- ggplot(musicology, aes(energy, danceability, color = artist.name)) +
  geom_point(size = 5, alpha = 0.4) +
  theme(text = element_text(family = "Bookman", color = "Gray25")) +
  labs(title = "Artists show different distributions in energy and danceability",
       y = "Danceability", 
       x = "Energy",
       subtitle = "The Script shows a lot of diversity") +
  guides(color = guide_legend(title = "Artist"))
#energy_dance
ggplotly(energy_dance)
```

***
When we analyze the danceability and energy of the tracks, there are noticeable differences between each of the artists. Notice that the figure shown here is limited in the axis as all the tracks group well within the current ranges.

For this analysis, The Script takes it all. Many of their tracks are spread between low and high energy _and_ danceability, thus showing a lot of variation. RADWIMPS skews towards a not-too-high-not-too-low danceability and energy whereas ONE OK ROCK shows much higher energy levels. These difference in energy between the two artists may stem from the genre, where ONE OK ROCK, as the name implies, plays many rock songs unlike RADWIMPS does. The danceability between them are similar however.

Day6 follows a line with low energy and danceability in their tracks, to tracks with high levels in both.

<iframe src="https://open.spotify.com/embed/playlist/4qytaIOVyWpUF2ncTXkjLZ?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" data-external="1"></iframe>

### Differences between Western and Eastern music

```{r}
musicology |>                    # All tracks
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) |>
  ggplot(                     # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = loudness,
      colour = mode
    )
  ) +
  geom_point() +              # Scatter plot.
  geom_rug(linewidth = 0.1) + # Add 'fringes' to show data distribution.
  geom_text(                  # Add text labels from above.
    aes(
      x = valence,
      y = energy,
      label = ""
    ),
    colour = "black",         # Override colour (not mode here).
    size = 3,                 # Override size (not loudness here).
    hjust = "left",           # Align left side of label with the point.
    vjust = "bottom",         # Align bottom of label with the point.
    nudge_x = -0.05,          # Nudge the label slightly left.
    nudge_y = 0.02            # Nudge the label slightly up.
  ) +
  facet_wrap(~artist.name) +    # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Paired"        # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasise loud.
    guide = "none"            # Remove the legend for size.
  ) +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    title = "Positivity between artists",
    x = "Valence",
    y = "Energy",
    colour = "Mode"
  )
```

***
Many of the artists choose for a major modality in their tracks, with DAY6 showing more of the minor kind. Furthermore the loudness seem to be similar across all artists, but we now have valence which does differ for each.

In this case The Script again shows a lot of variation in valence, but RADWIMPS and ONE OK ROCK skew more toward lower valence, although their tracks with higher valence are also noticeably louder. DAY6 shows a similar trend, and it may point to these three artists being similar because they are in the same continent, whereas The Script is on the other side of the world.

<iframe src="https://open.spotify.com/embed/playlist/4qytaIOVyWpUF2ncTXkjLZ?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" data-external="1"></iframe>

### Dynamic Time Warping (DTW) between similar songs

```{r}
## The Tallis Scholars
science_faith <-
  get_tidy_audio_analysis("0cnouzAiEjdjXB5xVVQ8Vo") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
## La Chapelle Royale
wasted_nights <-
  get_tidy_audio_analysis("39LyUQIy7idLxPdsjyZsxe") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


compmus_long_distance(
  science_faith |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  wasted_nights |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Science & Faith", y = "Wasted Nights", title = "Dynamic Time Warping between two similar songs", subtitle = "similarity based on energy and danceability") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL)
```

***
For a next analysis I decided to pick two songs from the "overal feel" plot that were similar in energy and danceability, these songs being Science & Faith (The Script) and Wasted Nights (ONE OK ROCK). Despite my expectations there seems to be solely dissimilarity between them, and there's hardly any vector or line present that displays anything meaningful. While Dynamic Time Warping can help understanding the difference between chroma vectors in two songs, it'll be important to know where and what songs to look into for proper analysis. What I've shown in this figure suggests that energy and danceability are perhaps not a major factor in regional difference, but rather similarity.


### Chromagram of We'll Be Alright

```{r, out.extra=c('allow="encrypted-media"', 'allowtransparency="true"', 'frameBorder="0"')}
alright <-
  get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

alright_timbre <-
  get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
```

```{r}
alright |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

***
In this part we will discuss one of the outliers from our first data plot. We'll Be Alright was written by RADWIMPS and shows a quite low level of danceability, with a value of 0.174, compared to all the other tracks. What is interesting to see is the overal distribution of the pitches; there is not so much a dominance. The next plot will show a timbregram which looks quite different.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7vSF7u4vWtZGrWCxTbAVaw?utm_source=generator&theme=0" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy" data-external="1"></iframe>
```

### Timbregram of We'll Be Alright

```{r}
alright_timbre |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

***
In the timbregram for We'll Be Alright we can see a dominance in magnitude toward pitches c01-c03. c02 has strong emphasis on certain timestamps while being present through the entire song otherwise. It's tricky to pinpoint the variations in the song. But in the case of c02, the timestamps where this is dominant is when the orchestral segments kick in, at 100, 190 and at 300 seconds when the outro happens.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7vSF7u4vWtZGrWCxTbAVaw?utm_source=generator&theme=0" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy" data-external="1"></iframe>
```


### Self-Similarity Matrices of We'll Be Alright

```{r hazes}
alright <-
  get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  alright |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  alright |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***
The chroma-based self-similarity matrix of We'll Be Alright shows something very interesting. There is nearly no similarity between any of the parts within the song!

Naturally you'd expect songs in general to show some level of similarity, or rather repetition, in different parts of the song. Do not be mistaken however, this is nothing like the case with a song like Bohemian Rhapsody, where the entire structure has little pattern. Rather, We'll Be Alright shows an incredible amount of variation while maintaining a very clear structure in the parts. For example, the first and the second chorus have the same melody, but the instrumentation and rhythm are entirely different.

On one hand it's unwise to identify it as an outlier since it still has strong pop music themes. On the other hand it's also an outlier of RADWIMPS themselves, and it's hardly representative for what their music is generally like. However that's something to look for in a manual analysis too.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7vSF7u4vWtZGrWCxTbAVaw?utm_source=generator&theme=0" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy" data-external="1"></iframe>
```

### Chordogram of We'll Be Alright

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r}
alright <-
  get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
alright |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***
Here we have a chordogram of the song We'll Be Alright. There's notably something interesting happening later on in the song. For the most part the chords look consistent. But if you listen to the time frame at around 200 seconds into the song, you will hear a very short change in key. In fact it changes twice there and serves as an embellishment.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7vSF7u4vWtZGrWCxTbAVaw?utm_source=generator&theme=0" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy" data-external="1"></iframe>
```


### Variation in different features for all tracks

```{r}
RADWIMPS <-
  musicology %>%
  filter(artist.name == "RADWIMPS") |>
  slice(1:30) |>
  add_audio_analysis()
thescript <-
  musicology %>%
  filter(artist.name == "The Script") |>
  slice(1:30) |>
  add_audio_analysis()
ONEOKROCK <-
  musicology %>%
  filter(artist.name == "ONE OK ROCK") |>
  slice(1:30) |>
  add_audio_analysis()
DAY6 <-
  musicology %>%
  filter(artist.name == "DAY6") |>
  slice(1:30) |>
  add_audio_analysis()
pop <-
  RADWIMPS |>
  mutate(genre = "RADWIMPS") |>
  bind_rows(thescript |> mutate(genre = "The Script")) |>
  bind_rows(ONEOKROCK |> mutate(genre = "ONE OK ROCK")) |>
  bind_rows(DAY6 |> mutate(genre = "DAY6"))

pop |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections) |>
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = genre,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Genre",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )
```

***
Here we have a plot with some advanced metrics. First if we look at RADWIMPS, we can see that many of their songs are higher in duration than other artists do. Songs that are higher in tempo seem to also have a higher standard deviation in tempo. In the section where we show tempograms you will see why this is the case. As for the other metrics, volume, mean tempo and loudness, they're quite balanced for each artist. You could say that they show a similar level of variation, at the same time the tracks aren't very unique in these metrics. However we will take a closer look at the different tempi as there are different distributions going on there.


### Histogram of tempi for different groups

```{r}
musicology <- get_playlist_audio_features("", "4qytaIOVyWpUF2ncTXkjLZ") %>%
  mutate(artist.name = map_chr(track.artists, function(x) x$name[1]))

tempigram <- ggplot(musicology, aes(tempo, color = artist.name)) +
  geom_histogram(fill = "Black") +
  facet_wrap(~artist.name) +
  theme_classic()

ggplotly(tempigram)
```
***
Here's a histogram showing the tempo of each track for every artist. Each artist clearly varies their tempo in their songs, and there isn't much significant to note in terms of how the tempi differ between the different artists. The Script has the majority of their songs at around 100 BPM while all the other artists don't show a particular preference. As you may notice, some tracks have a very high tempo (around 180 BPM). In the next section we'll explore some of these high tempo songs.


### outliers in tempo - and their tempograms

```{r}
par(mfrow=c(2,2))

zenzen <- get_tidy_audio_analysis("57Hk29LVcJAaGSFYQSuOKM")
alright <- get_tidy_audio_analysis("7vSF7u4vWtZGrWCxTbAVaw")
beginning <- get_tidy_audio_analysis("4f3nDjgqXurMryYBSp0TZD")
somebody <- get_tidy_audio_analysis("4aceMabp5rzZYoKKXsUffr")
  
tempi <-
  zenzen |>
  mutate(track = "Zenzenzense") |>
  bind_rows(alright |> mutate(track = "We'll Be Alright")) |>
  bind_rows(beginning |> mutate(track = "The Beginning")) |>
  bind_rows(somebody |> mutate(track = "누군가 필요해 I Need Somebody"))


somebody |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)",
       title = "누군가 필요해 I Need Somebody (Day6)") +
  theme_classic()


zenzen |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)",
       title = "Zenzenzense (RADWIMPS)") +
  theme_classic()

alright |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)",
       title = "We'll Be Alright (RADWIMPS)") +
  theme_classic()

beginning |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)",
       title = "The Beginning (ONE OK ROCK)") +
  theme_classic()
```

***
It's quite common to see slow songs on music streaming platforms, but what about songs with a tempo of over 190 BPM? As it happens, RADWIMPS has two songs around that tempo of which one of them is again We'll Be Alright, and the other Zenzenzense. ONE OK ROCK and Day6 also have a couple songs at very high BPM, namely, The Beginning and 누군가 필요해 I Need Somebody respectively. Let's take a look at their tempograms.

One thing you might notice is that The Beginning has a tempo that is very consistent throughout the song, and there is not much to observe other than the small spike at around 20 seconds. When listening to the song you'll find that it happens around the beginning of the transition from intro to verse, and it may feel like a 'kick start' into the song.

I Need Somebody is quite similar, but a lot more is going on at the start. In reality the tempogram might just be getting confused here; to a human one constant tempo is easily heard, but the beats are not consistent which may explain the plot. Otherwise the rest of the song seems to have a fixed tempo.

Both of the songs by RADWIMPS show interesting tempograms. While there is a clear consistent tempo present (shown by the long yellow line), the tempo also seems to be all over the place. For Zenzenzense this makes sense, there are a lot of small variations in rhythm even throughout the same segments in the song while the tempo stays the same. Ironically, likely because of this the song's danceability is relatively low, while its energy is very high as it's an up-beat song. The song We'll Be Alright is not nearly the same in this aspect, but still shows a similar all-over-the-place distribution of the tempo. This is a bit more difficult to explain however, it's not exactly rich in rhythm variation but rather it's rich in variation of everything; The song starts with simple guitar strums and singing, but it keeps switching to segments that either: transition to busier segments, change the entire instrumentation along with the rhythm through repeated segments (first and second chorus for example) or switch to a segment that's entirely orchestral or a mix with non-orchestral instruments. Given the other few plots we've seen about We'll Be Alright it makes sense to consider this song more of an outlier despite its popularity.

One last thing to note is that despite Spotify labeling these songs with very high tempo, the tempograms seem to show a different story; according to the plots the tempi are around 90 to 100 BPM instead of the Spotify-labeled 170 to 190 BPM.



Conclusion {.storyboard}
========================================


### What makes each region's music unique

```{r}
```

A lot can differ in music from different regions of the world, but similarities also show a good resemblance between artists and their music, and it makes it easier to judge what really makes the difference. We've seen that some artists prefer composing songs in major, and one in minor. We've also seen how valence, energy and danceability show different variations in each artist (which may point to the major differences!). What we can analyze with the available data is however quite limited; plently of features didn't show interesting numbers that affect how we see the music, and we don't have features with song structure and rhythm.

We have however explored chrome, chord-and timbregrams as well as analysed the tempo for different tracks. And it's interesting to see that what we see in these advanced plots also can be heard back in the respective songs, which shows how each track is unique in its visualization rather than just telling by hearing. We've also explored advanced metrics such as standard deviation and it shows interesting but also similar distributions between each artist.

As for how different artists from different regions in the world are unique in their culture, it may be a bit difficult to tell from only a few artists. It's a wise choice to pick some of the popular groups, as that will represent most of the population of the respective countries (or audience). But the challenge comes when all the artists show major similarities in for example genre and instrumentation as they bring noise to our analysis when we are actually trying to find clear trends in each region. Nevertheless, while popular artist represent a larger audience, the same artists can also have similar audiences, which is one thing we couldn't distinguish for; we know plenty about the tracks thanks to Spotify's data, however in a future work it may be insightful to analyse the audience as well, for example through machine learning methods such as clustering. There's a good chance that perhaps what each region in the world tends to listen to actually do so because of the things all artists have in common, and it may no longer be a regional preference to music but person to person. All the music in the world is after all accessible through almost any music streaming platform.